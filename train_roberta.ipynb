{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "To run this file on google colab, you need to run the code in this section to prepare the environment.\n",
        "\n",
        "!pip uninstall torch torchvision torchaudio\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -U bitsandbytes\n",
        "!pip install peft\n",
        "\n",
        "import os\n",
        "os._exit(00)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "If you want to run extract srl features on google colab, you need to run the code in this section.\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y make build-essential libssl-dev zlib1g-dev \\\n",
        "libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\n",
        "libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev \\\n",
        "liblzma-dev python-openssl git\n",
        "!curl https://pyenv.run | bash\n",
        "\n",
        "import os\n",
        "os.environ['PATH'] = f\"/root/.pyenv/bin:{os.environ['PATH']}\"\n",
        "!echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc\n",
        "!echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc\n",
        "!echo 'eval \"$(pyenv init --path)\"' >> ~/.bashrc\n",
        "!echo 'eval \"$(pyenv init -)\"' >> ~/.bashrc\n",
        "\n",
        "!apt-get install -y libffi-dev libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev\n",
        "\n",
        "!~/.pyenv/versions/3.7.12/bin/python --version\n",
        "\n",
        "!~/.pyenv/versions/3.7.12/bin/python -m pip install allennlp\n",
        "\n",
        "!~/.pyenv/versions/3.7.12/bin/python -c \"import allennlp; print(allennlp.__version__)\"\n",
        "\n",
        "!~/.pyenv/versions/3.7.12/bin/python -m pip install allennlp-models\n",
        "\n",
        "!~/.pyenv/versions/3.7.12/bin/python train.py --include-package allennlp_models.structured_prediction\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuqLiuNz0yRL",
        "outputId": "916a47d8-5af4-4cce-e868-0acd36f180b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-06 06:34:17.692706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741242857.713553   25642 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741242857.720083   25642 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-06 06:34:17.741364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Error using 8-bit quantization: \"normal_kernel_cpu\" not implemented for 'Char'\n",
            "Falling back to standard precision.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "Epoch {epoch + 1}/{epochs}\n",
            "Training:   0% 0/1865 [00:00<?, ?batch/s]Step 0, Loss: 0.7941054701805115\n",
            "Training:  16% 300/1865 [04:31<24:23,  1.07batch/s]Step 300, Loss: 0.6194016337394714\n",
            "Training:  32% 600/1865 [09:11<19:39,  1.07batch/s]Step 600, Loss: 0.3974526524543762\n",
            "Training:  48% 900/1865 [13:51<15:00,  1.07batch/s]Step 900, Loss: 0.27700433135032654\n",
            "Training:  64% 1200/1865 [18:31<10:21,  1.07batch/s]Step 1200, Loss: 0.2561279535293579\n",
            "Training:  80% 1500/1865 [23:11<05:40,  1.07batch/s]Step 1500, Loss: 0.19446738064289093\n",
            "Training:  97% 1800/1865 [27:50<01:00,  1.07batch/s]Step 1800, Loss: 0.3017812669277191\n",
            "Training: 100% 1865/1865 [28:51<00:00,  1.08batch/s]\n",
            "Epoch 1 Loss: 0.3867073793513526\n",
            "Dev:   0% 0/1139 [00:00<?, ?batch/s]Step 0, Loss: 0.5118662118911743\n",
            "Dev:  26% 300/1139 [02:03<05:44,  2.43batch/s]Step 300, Loss: 0.19116796553134918\n",
            "Dev:  53% 600/1139 [04:06<03:40,  2.45batch/s]Step 600, Loss: 0.03099827468395233\n",
            "Dev:  79% 900/1139 [06:08<01:37,  2.44batch/s]Step 900, Loss: 0.12694242596626282\n",
            "Dev: 100% 1139/1139 [07:46<00:00,  2.44batch/s]\n",
            "Dev Loss: 0.1581115926369776\n",
            "Confusion Matrix: [[31240  1933]\n",
            " [  553  2712]]\n",
            "Precision: [0.98260623 0.58385361]\n",
            "Recall: [0.94172972 0.83062787]\n",
            "F1: [0.96173383 0.68571429]\n",
            "Accuracy: 0.9317745211043417\n",
            "Macro Precision: 0.7832299200523354\n",
            "Macro Recall: 0.886178795657957\n",
            "Macro F1: 0.8237240578588361\n",
            "Test:   0% 0/1343 [00:00<?, ?batch/s]Step 0, Loss: 0.6146714687347412\n",
            "Test:  22% 300/1343 [02:03<07:13,  2.41batch/s]Step 300, Loss: 0.05669207498431206\n",
            "Test:  45% 600/1343 [04:07<05:06,  2.43batch/s]Step 600, Loss: 0.052899811416864395\n",
            "Test:  67% 900/1343 [06:11<03:02,  2.42batch/s]Step 900, Loss: 0.13525567948818207\n",
            "Test:  89% 1200/1343 [08:15<00:59,  2.42batch/s]Step 1200, Loss: 0.09999631345272064\n",
            "Test: 100% 1343/1343 [09:13<00:00,  2.43batch/s]\n",
            "Dev Loss: 0.1578401247627495\n",
            "Confusion Matrix: [[36833  2273]\n",
            " [  774  3068]]\n",
            "Precision: [0.97941873 0.57442427]\n",
            "Recall: [0.94187593 0.79854243]\n",
            "F1: [0.96028053 0.66819122]\n",
            "Accuracy: 0.929053739405793\n",
            "Macro Precision: 0.7769214951781072\n",
            "Macro Recall: 0.8702091763938071\n",
            "Macro F1: 0.8142358745143637\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "充分發揮 Colab 訂閱的價值",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
